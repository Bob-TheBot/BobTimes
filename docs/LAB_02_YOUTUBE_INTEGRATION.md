# Lab 2: Adding YouTube Data Source with Transcription

Welcome to Lab 2! In this lab, you'll extend the BobTimes system by adding YouTube as a new data source. You'll learn how to create custom tools, integrate external APIs, and enhance the reporter agents with video content analysis capabilities.

## üéâ Implementation Status: COMPLETED ‚úÖ

This lab has been **fully implemented** with the following features:
- ‚úÖ **YouTube Reporter Tool** (`libs/common/utils/youtube_tool.py`)
- ‚úÖ **YouTube Service** (`libs/common/utils/youtube_service.py`)
- ‚úÖ **YouTube Data Models** (`libs/common/utils/youtube_models.py`)
- ‚úÖ **Reporter Integration** (YouTube tool registered in `ReporterToolRegistry`)
- ‚úÖ **Configuration** (Field-based channel configuration in `.env.development`)
- ‚úÖ **Comprehensive Testing** (4 test files covering different scenarios)
- ‚úÖ **Two Operation Modes**: Topic extraction and video transcription
- ‚úÖ **Transcript-Only Mode**: Works without YouTube API quota
- ‚úÖ **Source Tracking**: Automatic `StorySource` creation for news attribution

## üéØ Lab Objectives

By the end of this lab, you will:
- ‚úÖ Create a YouTube tool that accepts user-provided channel lists
- ‚úÖ Extract recent video titles for topic discovery
- ‚úÖ Implement selective video transcription for content generation
- ‚úÖ Integrate the tool into the reporter agent workflow
- ‚úÖ Test the complete workflow: channels ‚Üí topics ‚Üí transcripts ‚Üí content
- ‚úÖ Understand how to extend the system with new data sources

## üìã Prerequisites

- ‚úÖ Completed Lab 1 (Basic setup and configuration)
- ‚úÖ Working DevContainer or local development environment
- ‚úÖ YouTube Data API v3 key (free from Google Cloud Console) - **Optional for transcript-only mode**
- ‚úÖ Understanding that transcripts come from YouTube's auto-generated captions
- ‚úÖ Basic understanding of the tool architecture

## üéØ Two Implementation Modes

This lab supports two modes of operation:

### üîì **Transcript-Only Mode (No API Key Required)**
- ‚úÖ Extract transcripts from specific video IDs
- ‚úÖ Full reporter agent integration
- ‚úÖ Source tracking for news generation
- ‚ùå Cannot discover new videos from channels
- ‚ùå Cannot browse channel content

### üîë **Full Mode (API Key Required)**
- ‚úÖ All transcript-only features
- ‚úÖ Discover recent videos from YouTube channels
- ‚úÖ Browse channel content by field (technology, science, etc.)
- ‚úÖ Automatic topic extraction from video titles
- ‚ö†Ô∏è Subject to YouTube API quota limits (10,000 units/day free)

## üöÄ Step 1: Setup YouTube Data API

### 1.1 Get YouTube Data API Key (FREE)

**üí∞ Cost Information:**
- YouTube Data API v3 is **FREE** up to 10,000 quota units per day
- Each video search costs ~100 units, each video details request costs ~1 unit
- This lab's usage will be well within the free tier (typically 50-100 videos per day)
- **No billing account required** for the free tier

1. **Go to Google Cloud Console**
   - Visit: https://console.cloud.google.com/
   - Create a new project or select existing one (free)

2. **Enable YouTube Data API v3**
   ```bash
   # In Google Cloud Console:
   # 1. Go to "APIs & Services" > "Library"
   # 2. Search for "YouTube Data API v3"
   # 3. Click "Enable" (no billing required for free tier)
   ```

3. **Create API Credentials**
   ```bash
   # In Google Cloud Console:
   # 1. Go to "APIs & Services" > "Credentials"
   # 2. Click "Create Credentials" > "API Key"
   # 3. Copy the generated API key
   # 4. (Recommended) Restrict the key to YouTube Data API v3 for security
   ```

**üìä Quota Usage Estimate for This Lab:**
- Search 3 channels √ó 5 videos each = ~300 quota units
- Get video details for 15 videos = ~15 quota units
- **Total per run: ~315 units** (well within 10,000 daily limit)
- You can run the lab **30+ times per day** within the free tier

**üÜì Transcript Access:**
- YouTube transcript fetching is **completely FREE** and has no quotas
- Uses the `youtube-transcript-api` Python library (no Google API key needed for transcripts)
- Only the video metadata requires the YouTube Data API key

**üîÑ Alternative: No-API Approach (Optional)**
If you prefer not to use Google APIs, you can modify the lab to:
- Use hardcoded video IDs instead of channel searches
- Focus only on transcript extraction (completely free)
- Skip video metadata and channel information
- This approach requires **zero API keys** and has **zero costs**

### 1.2 Configure API Keys and Settings

Add your API key to the secrets file and channel configuration to the environment file:

```yaml
# In libs/common/secrets.yaml
llm_providers:
  # ... existing providers ...

# YouTube Data Source Configuration (API Key only)
youtube:
  api_key: "your_youtube_data_api_key_here"
```

### 1.3 Update Environment Configuration

```bash
# In libs/.env.development
# Add YouTube configuration
YOUTUBE_ENABLED=true
YOUTUBE_MAX_VIDEOS_PER_CHANNEL=1
YOUTUBE_DAYS_BACK=7
YOUTUBE_CONCURRENT_REQUESTS=3
YOUTUBE_REQUEST_DELAY_MS=100

# YouTube Channel Configuration (comma-separated channel IDs)
# üéØ IMPORTANT: Use direct channel IDs to avoid API quota for username resolution
# Format: Just the channel ID (UCxxxxx) or full URL with channel ID

# Technology channels - using direct channel IDs to avoid API quota
YOUTUBE_CHANNELS_TECHNOLOGY="UCXuqSBlHAE6Xw-yeJA0Tunw,UC8QMvQrV1bsK7WO37QpSxSg,UCBJycsmduvYEL83R_U4JriQ"

# Science channels - using direct channel IDs to avoid API quota
YOUTUBE_CHANNELS_SCIENCE="UCsXVk37bltHxD1rDPwtNM8Q,UC6nSFpj9HTCZ5t-N3Rm3-HA,UCHnyfMqiRRG1u-2MsSQLbXA"

# Economics channels - using direct channel IDs to avoid API quota
YOUTUBE_CHANNELS_ECONOMICS="UCZ4AMrDcNrfy3X6nsU8-rPg,UCDXTQ8nWmx_EhZ2v-kp7QxA"

# Sports channels (optional)
YOUTUBE_CHANNELS_SPORTS="UCqFMzb-4AUf6WAIbhOJ5P8w,UCWWbZ8z9GwvbR_7NUjNYJdw"
```

**üí° Channel ID vs Username URLs:**
- ‚úÖ **Direct Channel IDs** (like `UCXuqSBlHAE6Xw-yeJA0Tunw`) - No API quota for resolution
- ‚ùå **Username URLs** (like `@GoogleDevelopers`) - Requires API quota for resolution
- üîç **How to find Channel IDs**: Visit the channel page, view source, search for "channelId"

## üõ†Ô∏è Step 2: Create YouTube Tool Infrastructure

### 2.1 Install Required Dependencies

```bash
# In DevContainer terminal
uv add youtube-transcript-api
uv add google-api-python-client
# Note: We only use YouTube's built-in transcription API, no additional models needed
```

### 2.2 Create YouTube Data Models

Create the file `libs/common/utils/youtube_models.py`:

```python
"""YouTube data models for the BobTimes system."""

from datetime import datetime
from enum import StrEnum
from typing import Optional

from pydantic import BaseModel, Field


class YouTubeField(StrEnum):
    """YouTube content fields."""
    TECHNOLOGY = "technology"
    SCIENCE = "science"
    ECONOMICS = "economics"
    SPORTS = "sports"


class VideoQuality(StrEnum):
    """Video quality options."""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class TranscriptionMethod(StrEnum):
    """Transcription method options."""
    YOUTUBE_AUTO = "youtube_auto"  # Use YouTube's auto-generated captions (primary method)


class YouTubeVideo(BaseModel):
    """Represents a YouTube video with metadata."""
    video_id: str = Field(description="YouTube video ID")
    title: str = Field(description="Video title")
    description: str = Field(description="Video description")
    channel_id: str = Field(description="Channel ID")
    channel_title: str = Field(description="Channel name")
    published_at: datetime = Field(description="Publication date")
    duration: str = Field(description="Video duration (ISO 8601)")
    view_count: int = Field(default=0, description="Number of views")
    like_count: int = Field(default=0, description="Number of likes")
    url: str = Field(description="Full YouTube URL")
    thumbnail_url: Optional[str] = Field(None, description="Video thumbnail URL")


class VideoTranscript(BaseModel):
    """Represents a video transcript."""
    video_id: str = Field(description="YouTube video ID")
    transcript: str = Field(description="Full transcript text")
    language: str = Field(default="en", description="Transcript language")
    method: TranscriptionMethod = Field(description="How transcript was obtained")
    confidence: Optional[float] = Field(None, description="Transcription confidence score")
    word_count: int = Field(description="Number of words in transcript")
    created_at: datetime = Field(default_factory=datetime.now)


class YouTubeChannelInfo(BaseModel):
    """Represents YouTube channel information."""
    channel_id: str = Field(description="YouTube channel ID")
    title: str = Field(description="Channel title")
    description: str = Field(description="Channel description")
    subscriber_count: int = Field(default=0, description="Number of subscribers")
    video_count: int = Field(default=0, description="Total videos")
    url: str = Field(description="Channel URL")


class YouTubeSearchParams(BaseModel):
    """Parameters for YouTube search operations."""
    channel_ids: list[str] = Field(description="User-provided list of channel IDs to search")
    max_videos_per_channel: int = Field(default=5, description="Max videos per channel")
    days_back: int = Field(default=7, description="How many days back to search")
    extract_topics_only: bool = Field(default=False, description="Only extract titles for topics")
    include_transcripts: bool = Field(default=False, description="Fetch transcripts for content generation")
    specific_video_ids: list[str] = Field(default_factory=list, description="Specific videos to transcribe")
    transcription_method: TranscriptionMethod = Field(
        default=TranscriptionMethod.YOUTUBE_AUTO,
        description="Preferred transcription method"
    )


class YouTubeSearchResult(BaseModel):
    """Result from YouTube search operation."""
    success: bool = Field(description="Whether the search was successful")
    videos: list[YouTubeVideo] = Field(default_factory=list, description="Found videos")
    transcripts: dict[str, VideoTranscript] = Field(
        default_factory=dict, 
        description="Video transcripts keyed by video_id"
    )
    channels: dict[str, YouTubeChannelInfo] = Field(
        default_factory=dict,
        description="Channel info keyed by channel_id"
    )
    total_videos: int = Field(default=0, description="Total videos found")
    error: Optional[str] = Field(None, description="Error message if failed")
```

## üîß Step 3: Implement YouTube Tool

### 3.1 Create YouTube Service

Create the file `libs/common/utils/youtube_service.py`:

```python
"""YouTube service for fetching videos and transcripts."""

import asyncio
from datetime import datetime, timedelta
from typing import Optional

from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound

from core.config_service import ConfigService
from core.logging_service import get_logger
from .youtube_models import (
    YouTubeVideo, VideoTranscript, YouTubeChannelInfo,
    YouTubeSearchParams, YouTubeSearchResult, TranscriptionMethod, YouTubeField
)

logger = get_logger(__name__)


class YouTubeService:
    """Service for interacting with YouTube Data API and transcripts."""

    def __init__(self, config_service: ConfigService):
        """Initialize YouTube service with configuration."""
        self.config_service = config_service
        self.api_key = config_service.get("youtube.api_key")

        if not self.api_key:
            raise ValueError("YouTube API key not found in configuration")

        # Initialize YouTube API client
        self.youtube = build('youtube', 'v3', developerKey=self.api_key)

        logger.info("YouTube service initialized successfully")

    def get_channels_for_field(self, field: YouTubeField) -> list[str]:
        """Get channel IDs for a specific field from environment configuration.

        Supports both YouTube URLs and channel IDs. URLs are automatically converted to channel IDs.
        """
        env_key = f"youtube_channels_{field.value}"
        channels_str = self.config_service.get(env_key, "")

        if not channels_str:
            logger.warning(f"No channels configured for field: {field}")
            return []

        # Split comma-separated entries and clean whitespace
        raw_channels = [channel.strip() for channel in channels_str.split(",") if channel.strip()]

        # Convert URLs to channel IDs
        channel_ids = []
        for channel in raw_channels:
            channel_id = self._extract_channel_id(channel)
            if channel_id:
                channel_ids.append(channel_id)
            else:
                logger.warning(f"Could not extract channel ID from: {channel}")

        logger.info(f"Found {len(channel_ids)} valid channels for field {field}")
        return channel_ids

    def _extract_channel_id(self, channel_input: str) -> str | None:
        """Extract channel ID from URL or return as-is if already a channel ID."""
        import re

        # If it's already a channel ID (starts with UC and is 24 chars), return as-is
        if channel_input.startswith("UC") and len(channel_input) == 24:
            return channel_input

        # Extract from various YouTube URL formats
        url_patterns = [
            r"youtube\.com/channel/([a-zA-Z0-9_-]{24})",  # /channel/UCxxxxx
            r"youtube\.com/@([a-zA-Z0-9_.-]+)",           # /@username
            r"youtube\.com/c/([a-zA-Z0-9_.-]+)",          # /c/username
            r"youtube\.com/user/([a-zA-Z0-9_.-]+)",       # /user/username
        ]

        for pattern in url_patterns:
            match = re.search(pattern, channel_input)
            if match:
                extracted = match.group(1)
                # If it's a direct channel ID, return it
                if extracted.startswith("UC") and len(extracted) == 24:
                    return extracted
                # Otherwise, it's a username that needs to be resolved via API
                return self._resolve_username_to_channel_id(extracted)

        logger.warning(f"Could not parse channel input: {channel_input}")
        return None

    def _resolve_username_to_channel_id(self, username: str) -> str | None:
        """Resolve a username to channel ID using YouTube API."""
        try:
            # Try to search for the channel by username
            response = self.youtube.search().list(
                part='snippet',
                q=username,
                type='channel',
                maxResults=1
            ).execute()

            if response['items']:
                return response['items'][0]['snippet']['channelId']

            logger.warning(f"Could not resolve username to channel ID: {username}")
            return None

        except Exception as e:
            logger.error(f"Error resolving username {username}: {e}")
            return None
    
    async def search_channel_videos(self, params: YouTubeSearchParams) -> YouTubeSearchResult:
        """Search for recent videos from specified channels."""
        try:
            all_videos = []
            all_transcripts = {}
            channel_info = {}
            
            # Calculate date threshold
            since_date = datetime.now() - timedelta(days=params.days_back)
            
            for channel_id in params.channel_ids:
                logger.info(f"Fetching videos from channel: {channel_id}")
                
                # Get channel info
                channel_data = await self._get_channel_info(channel_id)
                if channel_data:
                    channel_info[channel_id] = channel_data
                
                # Get recent videos
                videos = await self._get_channel_videos(
                    channel_id, 
                    params.max_videos_per_channel,
                    since_date
                )
                
                all_videos.extend(videos)
                
                # Get transcripts if requested
                if params.include_transcripts:
                    for video in videos:
                        transcript = await self._get_video_transcript(
                            video.video_id,
                            params.transcription_method
                        )
                        if transcript:
                            all_transcripts[video.video_id] = transcript
            
            return YouTubeSearchResult(
                success=True,
                videos=all_videos,
                transcripts=all_transcripts,
                channels=channel_info,
                total_videos=len(all_videos)
            )
            
        except Exception as e:
            logger.error(f"YouTube search failed: {e}")
            return YouTubeSearchResult(
                success=False,
                error=str(e)
            )
    
    async def _get_channel_info(self, channel_id: str) -> Optional[YouTubeChannelInfo]:
        """Get information about a YouTube channel."""
        try:
            response = self.youtube.channels().list(
                part='snippet,statistics',
                id=channel_id
            ).execute()
            
            if not response['items']:
                logger.warning(f"Channel not found: {channel_id}")
                return None
            
            item = response['items'][0]
            snippet = item['snippet']
            stats = item.get('statistics', {})
            
            return YouTubeChannelInfo(
                channel_id=channel_id,
                title=snippet['title'],
                description=snippet.get('description', ''),
                subscriber_count=int(stats.get('subscriberCount', 0)),
                video_count=int(stats.get('videoCount', 0)),
                url=f"https://www.youtube.com/channel/{channel_id}"
            )
            
        except Exception as e:
            logger.error(f"Failed to get channel info for {channel_id}: {e}")
            return None
    
    async def _get_channel_videos(
        self, 
        channel_id: str, 
        max_results: int,
        since_date: datetime
    ) -> list[YouTubeVideo]:
        """Get recent videos from a channel."""
        try:
            # Search for videos from the channel
            search_response = self.youtube.search().list(
                part='id,snippet',
                channelId=channel_id,
                type='video',
                order='date',
                maxResults=max_results,
                publishedAfter=since_date.isoformat() + 'Z'
            ).execute()
            
            video_ids = [item['id']['videoId'] for item in search_response['items']]
            
            if not video_ids:
                return []
            
            # Get detailed video information
            videos_response = self.youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=','.join(video_ids)
            ).execute()
            
            videos = []
            for item in videos_response['items']:
                video = self._parse_video_item(item)
                if video:
                    videos.append(video)
            
            return videos
            
        except Exception as e:
            logger.error(f"Failed to get videos for channel {channel_id}: {e}")
            return []
    
    def _parse_video_item(self, item: dict) -> Optional[YouTubeVideo]:
        """Parse a video item from YouTube API response."""
        try:
            snippet = item['snippet']
            stats = item.get('statistics', {})
            content_details = item.get('contentDetails', {})
            
            return YouTubeVideo(
                video_id=item['id'],
                title=snippet['title'],
                description=snippet.get('description', ''),
                channel_id=snippet['channelId'],
                channel_title=snippet['channelTitle'],
                published_at=datetime.fromisoformat(snippet['publishedAt'].replace('Z', '+00:00')),
                duration=content_details.get('duration', 'PT0S'),
                view_count=int(stats.get('viewCount', 0)),
                like_count=int(stats.get('likeCount', 0)),
                url=f"https://www.youtube.com/watch?v={item['id']}",
                thumbnail_url=snippet.get('thumbnails', {}).get('high', {}).get('url')
            )
            
        except Exception as e:
            logger.error(f"Failed to parse video item: {e}")
            return None
    
    async def _get_video_transcript(
        self,
        video_id: str,
        method: TranscriptionMethod
    ) -> Optional[VideoTranscript]:
        """Get transcript for a video using YouTube's auto-generated captions."""
        try:
            # Only support YouTube's built-in transcription
            if method == TranscriptionMethod.YOUTUBE_AUTO:
                return await self._get_youtube_transcript(video_id)
            else:
                logger.warning(f"Unsupported transcription method: {method}. Only YouTube auto-captions supported.")
                return None

        except Exception as e:
            logger.error(f"Failed to get transcript for {video_id}: {e}")
            return None
    
    async def _get_youtube_transcript(self, video_id: str) -> Optional[VideoTranscript]:
        """Get transcript using YouTube's auto-generated captions."""
        try:
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
            
            # Combine all transcript segments
            full_transcript = ' '.join([entry['text'] for entry in transcript_list])
            
            return VideoTranscript(
                video_id=video_id,
                transcript=full_transcript,
                language='en',
                method=TranscriptionMethod.YOUTUBE_AUTO,
                word_count=len(full_transcript.split())
            )
            
        except (TranscriptsDisabled, NoTranscriptFound) as e:
            logger.warning(f"No transcript available for {video_id}: {e}")
            return None
    
    # Note: We only use YouTube's built-in transcription API
    # No additional transcription methods are implemented in this lab
```

This is the foundation of our YouTube integration. Now let's create the reporter tool and integrate it into the agent workflow.

## üîß Step 4: Create YouTube Reporter Tool

### 4.1 Create YouTube Reporter Tool

Create the file `libs/common/utils/youtube_tool.py`:

```python
"""YouTube tool for reporter agents."""

from datetime import datetime
from typing import Any

from agents.models.story_models import StorySource
from agents.tools.base_tool import BaseTool
from core.config_service import ConfigService
from core.llm_service import ModelSpeed
from core.logging_service import get_logger
from pydantic import BaseModel, Field

from .youtube_models import YouTubeSearchParams, YouTubeSearchResult, TranscriptionMethod, YouTubeField
from .youtube_service import YouTubeService

logger = get_logger(__name__)


class YouTubeToolParams(BaseModel):
    """Parameters for YouTube tool operations."""
    channel_ids: list[str] = Field(
        default_factory=list,
        description="User-provided list of YouTube channel URLs or IDs (optional if field is specified)"
    )
    field: YouTubeField | None = Field(
        default=None,
        description="Field name to get channels from env config"
    )
    max_videos_per_channel: int = Field(default=3, description="Max videos per channel")
    days_back: int = Field(default=7, description="Days to look back for videos")
    operation: str = Field(default="topics", description="Operation: 'topics' or 'transcribe'")
    specific_video_ids: list[str] = Field(
        default_factory=list,
        description="Specific video IDs to transcribe (for 'transcribe' operation)"
    )


class YouTubeToolResult(BaseModel):
    """Result from YouTube tool execution."""
    success: bool = Field(description="Whether the operation was successful")
    operation: str = Field(description="Operation performed: 'topics' or 'transcribe'")
    videos_found: int = Field(default=0, description="Number of videos found")
    channels_searched: int = Field(default=0, description="Number of channels searched")
    transcripts_obtained: int = Field(default=0, description="Number of transcripts obtained")
    topics_extracted: list[str] = Field(default_factory=list, description="Video titles as potential topics")
    sources: list[StorySource] = Field(default_factory=list, description="YouTube video sources for story tracking")
    summary: str = Field(description="Summary of findings")
    detailed_results: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Detailed video information with optional transcripts"
    )
    error: str | None = Field(None, description="Error message if failed")


class YouTubeReporterTool(BaseTool):
    """YouTube data source tool for reporter agents."""

    def __init__(self, config_service: ConfigService | None = None):
        """Initialize YouTube tool."""
        name = "youtube_search"
        description = f"""
Extract topics from YouTube channels or transcribe specific videos for content generation.

PARAMETER SCHEMA:
{YouTubeToolParams.model_json_schema()}

CORRECT USAGE EXAMPLES:
# Extract topics from specific channels (URLs or IDs)
{{"channel_ids": ["https://www.youtube.com/@GoogleDevelopers", "https://www.youtube.com/@LinusTechTips"], "operation": "topics", "days_back": 7}}

# Extract topics from field-configured channels
{{"field": "technology", "operation": "topics", "days_back": 7}}

# Transcribe specific videos for content
{{"operation": "transcribe", "specific_video_ids": ["dQw4w9WgXcQ", "oHg5SJYRHA0"]}}

# Get recent videos from field and transcribe them
{{"field": "science", "operation": "transcribe", "max_videos_per_channel": 2}}

# Mix of direct channels and field channels
{{"channel_ids": ["https://www.youtube.com/@3Blue1Brown"], "field": "science", "operation": "topics"}}

USAGE GUIDELINES:
- ALWAYS provide channel_ids list (user-specified channels)
- Use operation="topics" to extract video titles for topic discovery
- Use operation="transcribe" to get full transcripts for content generation
- Set days_back to control how recent the videos should be (1-30 days)
- Use specific_video_ids for targeted transcription of known videos
- Max 5 videos per channel recommended for performance

RETURNS:
- operation: The operation performed ("topics" or "transcribe")
- topics_extracted: List of video titles (for topic discovery)
- detailed_results: Full video data with optional transcripts
- summary: Brief summary of findings

This tool supports two workflows:
1. Topic Discovery: Extract recent video titles from user-specified channels
2. Content Generation: Transcribe videos for detailed content analysis
"""
        super().__init__(name=name, description=description)
        self.params_model = YouTubeToolParams
        self.config_service = config_service or ConfigService()

        # Initialize YouTube service
        try:
            self.youtube_service = YouTubeService(self.config_service)
        except Exception as e:
            logger.error(f"Failed to initialize YouTube service: {e}")
            self.youtube_service = None

    async def execute(self, params: BaseModel, model_speed: ModelSpeed = ModelSpeed.FAST) -> YouTubeToolResult:
        """Execute YouTube search with structured params."""
        if not isinstance(params, YouTubeToolParams):
            return YouTubeToolResult(
                success=False,
                error=f"Expected YouTubeToolParams, got {type(params)}"
            )

        if not self.youtube_service:
            return YouTubeToolResult(
                success=False,
                error="YouTube service not available - check API key configuration"
            )

        try:
            # Determine channel IDs to use
            channel_ids = params.channel_ids.copy() if params.channel_ids else []

            # If field is specified, get channels from environment configuration
            if params.field is not None:
                field_channels = self.youtube_service.get_channels_for_field(params.field)
                channel_ids.extend(field_channels)

            # Convert any URLs to channel IDs
            resolved_channel_ids = []
            for channel in channel_ids:
                channel_id = self.youtube_service._extract_channel_id(channel)
                if channel_id:
                    resolved_channel_ids.append(channel_id)
                else:
                    logger.warning(f"Could not resolve channel: {channel}")

            # Validate we have channels to work with
            if not resolved_channel_ids and params.operation == "topics":
                return YouTubeToolResult(
                    success=False,
                    operation=params.operation,
                    error="No valid channel IDs found. Please specify either channel_ids parameter or field parameter."
                )

            # Update params with resolved channel IDs
            params.channel_ids = resolved_channel_ids

            # Handle different operations
            if params.operation == "topics":
                return await self._extract_topics(params)
            elif params.operation == "transcribe":
                return await self._transcribe_videos(params)
            else:
                return YouTubeToolResult(
                    success=False,
                    operation=params.operation,
                    error=f"Unknown operation: {params.operation}. Use 'topics' or 'transcribe'."
                )

            # Execute search
            logger.info(f"Searching YouTube for {params.field} content...")
            result = await self.youtube_service.search_channel_videos(search_params)

            if not result.success:
                return YouTubeToolResult(
                    success=False,
                    error=result.error or "YouTube search failed"
                )

            # Process results
            detailed_results = []
            for video in result.videos:
                video_data = {
                    "title": video.title,
                    "channel": video.channel_title,
                    "published": video.published_at.isoformat(),
                    "url": video.url,
                    "description": video.description[:500] + "..." if len(video.description) > 500 else video.description,
                    "views": video.view_count,
                    "duration": video.duration
                }

                # Add transcript if available
                if video.video_id in result.transcripts:
                    transcript = result.transcripts[video.video_id]
                    video_data["transcript"] = transcript.transcript
                    video_data["transcript_word_count"] = transcript.word_count

                detailed_results.append(video_data)

            # Create summary
            summary = self._create_summary(result, params.field)

            return YouTubeToolResult(
                success=True,
                videos_found=len(result.videos),
                channels_searched=len(channel_ids),
                transcripts_obtained=len(result.transcripts),
                summary=summary,
                detailed_results=detailed_results
            )

        except Exception as e:
            logger.error(f"YouTube tool execution failed: {e}")
            return YouTubeToolResult(
                success=False,
                operation=params.operation,
                error=str(e)
            )

    async def _extract_topics(self, params: YouTubeToolParams) -> YouTubeToolResult:
        """Extract video titles as potential topics from user-specified channels."""
        try:
            # Create search parameters for topic extraction
            search_params = YouTubeSearchParams(
                channel_ids=params.channel_ids,
                max_videos_per_channel=params.max_videos_per_channel,
                days_back=params.days_back,
                extract_topics_only=True,
                include_transcripts=False,  # Don't need transcripts for topic extraction
                transcription_method=TranscriptionMethod.YOUTUBE_AUTO
            )

            # Execute search
            logger.info(f"Extracting topics from {len(params.channel_ids)} channels...")
            result = await self.youtube_service.search_channel_videos(search_params)

            if not result.success:
                return YouTubeToolResult(
                    success=False,
                    operation="topics",
                    error=result.error or "Failed to extract topics"
                )

            # Extract topics (video titles)
            topics = [video.title for video in result.videos]

            # Create StorySource objects for source tracking
            sources = []
            for video in result.videos:
                source = StorySource(
                    url=video.url,
                    title=video.title,
                    summary=f"YouTube video from {video.channel_title}: {video.description[:200]}..." if video.description else f"YouTube video from {video.channel_title}",
                    accessed_at=datetime.now()
                )
                sources.append(source)

            # Create summary
            summary = f"Extracted {len(topics)} potential topics from {len(params.channel_ids)} channels"

            return YouTubeToolResult(
                success=True,
                operation="topics",
                videos_found=len(result.videos),
                channels_searched=len(params.channel_ids),
                topics_extracted=topics,
                sources=sources,
                summary=summary,
                detailed_results=[{
                    "title": video.title,
                    "channel": video.channel_title,
                    "published": video.published_at.isoformat(),
                    "url": video.url,
                    "views": video.view_count,
                    "description": video.description[:200] + "..." if len(video.description) > 200 else video.description
                } for video in result.videos]
            )

        except Exception as e:
            logger.error(f"Topic extraction failed: {e}")
            return YouTubeToolResult(
                success=False,
                operation="topics",
                error=str(e)
            )

    async def _transcribe_videos(self, params: YouTubeToolParams) -> YouTubeToolResult:
        """Transcribe specific videos or recent videos from channels for content generation."""
        try:
            videos_to_transcribe = []

            # If specific video IDs provided, use those
            if params.specific_video_ids:
                # Create dummy video objects for specific IDs
                for video_id in params.specific_video_ids:
                    videos_to_transcribe.append(video_id)
            else:
                # Get recent videos from channels first
                search_params = YouTubeSearchParams(
                    channel_ids=params.channel_ids,
                    max_videos_per_channel=params.max_videos_per_channel,
                    days_back=params.days_back,
                    extract_topics_only=False,
                    include_transcripts=False,  # We'll get transcripts separately
                    transcription_method=TranscriptionMethod.YOUTUBE_AUTO
                )

                result = await self.youtube_service.search_channel_videos(search_params)
                if not result.success:
                    return YouTubeToolResult(
                        success=False,
                        operation="transcribe",
                        error=result.error or "Failed to find videos to transcribe"
                    )

                videos_to_transcribe = [video.video_id for video in result.videos]

            # Now get transcripts for the videos
            transcripts = {}
            detailed_results = []
            sources = []

            for video_id in videos_to_transcribe:
                transcript = await self.youtube_service._get_video_transcript(
                    video_id,
                    TranscriptionMethod.YOUTUBE_AUTO
                )

                video_url = f"https://www.youtube.com/watch?v={video_id}"

                if transcript:
                    transcripts[video_id] = transcript
                    detailed_results.append({
                        "video_id": video_id,
                        "url": video_url,
                        "transcript": transcript.transcript,
                        "word_count": transcript.word_count,
                        "language": transcript.language
                    })

                    # Create StorySource for transcribed video
                    source = StorySource(
                        url=video_url,
                        title=f"YouTube Video {video_id}",  # We might not have title for specific video IDs
                        summary=f"YouTube video transcript ({transcript.word_count} words): {transcript.transcript[:200]}..." if len(transcript.transcript) > 200 else transcript.transcript,
                        accessed_at=datetime.now()
                    )
                    sources.append(source)

            summary = f"Transcribed {len(transcripts)} videos out of {len(videos_to_transcribe)} requested"

            return YouTubeToolResult(
                success=True,
                operation="transcribe",
                videos_found=len(videos_to_transcribe),
                transcripts_obtained=len(transcripts),
                sources=sources,
                summary=summary,
                detailed_results=detailed_results
            )

        except Exception as e:
            logger.error(f"Video transcription failed: {e}")
            return YouTubeToolResult(
                success=False,
                operation="transcribe",
                error=str(e)
            )
```

### 4.2 Register YouTube Tool with Reporter Registry

Now we need to add the YouTube tool to the reporter tool registry. Update the file `libs/common/agents/reporter_agent/reporter_tools.py`:

```python
# Add this import at the top
from utils.youtube_tool import YouTubeReporterTool

# Update the ReporterToolRegistry class __init__ method
class ReporterToolRegistry:
    """Registry for reporter tools with automatic schema generation."""

    def __init__(self, config_service: ConfigService | None = None) -> None:
        """Initialize the tool registry with reporter-specific tools."""
        self.config_service = config_service or ConfigService()

        self.tools = {
            "search": ReporterSearchTool(),
            "scrape": ReporterScraperTool(),
            "youtube_search": YouTubeReporterTool(self.config_service)  # Add YouTube tool
        }

    # Add method to handle YouTube sources (similar to convert_search_results_to_sources)
    @staticmethod
    def extract_youtube_sources(youtube_result: YouTubeToolResult) -> list[StorySource]:
        """Extract StorySource objects from YouTube tool results.

        Args:
            youtube_result: YouTubeToolResult object

        Returns:
            List of StorySource objects
        """
        # Use Pydantic model validation instead of hasattr
        try:
            return youtube_result.sources if youtube_result.sources else []
        except AttributeError:
            logger.warning("YouTubeToolResult missing sources field")
            return []

# Update the reporter executor to handle YouTube sources
# In libs/common/agents/reporter_agent/reporter_executor.py, add this logic:

async def _execute_tool_call(self, tool_call: StoryToolCall, state: ReporterState) -> ReporterToolResult:
    """Execute a tool call and update state with results and sources."""
    # ... existing code ...

    # After executing the tool, check if it's YouTube tool and extract sources
    if tool_call.name == "youtube_search" and isinstance(result, YouTubeToolResult):
        # Add YouTube sources to state
        youtube_sources = ReporterToolRegistry.extract_youtube_sources(result)
        for source in youtube_sources:
            if source.url not in [s.url for s in state.sources]:
                state.sources.append(source)

        logger.info(f"Added {len(youtube_sources)} YouTube sources to story sources")

    # ... rest of existing code ...
```

## üß™ Step 5: Test YouTube Integration

### 5.1 Create YouTube Test Script

Create a test script to verify the YouTube integration works:

```python
# Create file: test_youtube_integration.py
"""Test script for YouTube integration."""

import asyncio
from core.config_service import ConfigService
from utils.youtube_tool import YouTubeReporterTool, YouTubeToolParams
from utils.youtube_models import YouTubeField


async def test_youtube_tool():
    """Test the YouTube tool with different operations."""
    config_service = ConfigService()
    youtube_tool = YouTubeReporterTool(config_service)

    # Test 1: Extract topics from field-configured channels
    print("üîç Testing Topic Extraction...")
    topic_params = YouTubeToolParams(
        field=YouTubeField.TECHNOLOGY,
        operation="topics",
        days_back=7,
        max_videos_per_channel=3
    )

    result = await youtube_tool.execute(topic_params)

    if result.success:
        print(f"‚úÖ Operation: {result.operation}")
        print(f"‚úÖ Found {result.videos_found} videos from {result.channels_searched} channels")
        print(f"üìã Topics extracted: {len(result.topics_extracted)}")
        print(f"üîó Sources created: {len(result.sources)}")
        print(f"üìÑ Summary: {result.summary}")

        # Show first few topics
        if result.topics_extracted:
            print("\nüéØ Sample Topics:")
            for i, topic in enumerate(result.topics_extracted[:3]):
                print(f"   {i+1}. {topic}")

        # Show source tracking
        if result.sources:
            print("\nüìö Sources for Story Tracking:")
            for i, source in enumerate(result.sources[:2]):
                print(f"   {i+1}. {source.title}")
                print(f"      URL: {source.url}")
                print(f"      Summary: {source.summary[:100]}...")
    else:
        print(f"‚ùå Topic extraction failed: {result.error}")

    # Test 2: Transcribe specific videos
    print("\nüé¨ Testing Video Transcription...")
    transcribe_params = YouTubeToolParams(
        operation="transcribe",
        specific_video_ids=["dQw4w9WgXcQ"]  # Example video ID
    )

    transcribe_result = await youtube_tool.execute(transcribe_params)

    if transcribe_result.success:
        print(f"‚úÖ Operation: {transcribe_result.operation}")
        print(f"‚úÖ Transcripts obtained: {transcribe_result.transcripts_obtained}")
        print(f"üìÑ Summary: {transcribe_result.summary}")

        # Show transcript preview
        if transcribe_result.detailed_results:
            first_transcript = transcribe_result.detailed_results[0]
            print(f"\nüìù Transcript preview: {first_transcript['transcript'][:200]}...")
    else:
        print(f"‚ùå Transcription failed: {transcribe_result.error}")


if __name__ == "__main__":
    asyncio.run(test_youtube_tool())
```

### 5.2 Run the Test

```bash
# In DevContainer terminal
python test_youtube_integration.py
```

## üîß Step 6: Configure Field-Specific Channels

### 6.1 Update Channel Configuration

Add more comprehensive channel lists to your `.env.development` file:

```bash
# Enhanced YouTube channel configuration in libs/.env.development
# Technology channels (comma-separated)
YOUTUBE_CHANNELS_TECHNOLOGY="UC_x5XG1OV2P6uZZ5FSM9Ttw,UCXuqSBlHAE6Xw-yeJA0Tunw,UC4QZ_LsYcvcq7qOsOhpAX4A,UCld68syR8Wi-GY_n4CaoJGA,UCVls1GmFKf6WlTraIb_IaJg"

# Science channels (comma-separated)
YOUTUBE_CHANNELS_SCIENCE="UCsXVk37bltHxD1rDPwtNM8Q,UC6nSFpj9HTCZ5t-N3Rm3-HA,UCHnyfMqiRRG1u-2MsSQLbXA,UC7_gcs09iThXybpVgjHZ_7g,UCtYLUTtgS3k1Fg4y5tAhLbw"

# Economics channels (comma-separated)
YOUTUBE_CHANNELS_ECONOMICS="UCfM3zsQsOnfWNUppiycmBuw,UC0p5jTq6Xx_DosDFxVXnWaQ,UCZ4AMrDcNrfy3X6nsU8-rPg,UC-uWLJbmXH1KbCPHmtG-3MQ,UCcunJy13-KFJNd_DkqkUeEg"

# Sports channels (comma-separated, optional)
YOUTUBE_CHANNELS_SPORTS="UCqFMzb-4AUf6WAIbhOJ5P8w,UCWWbZ8z9GwvbR_7NUjNYJdw"

# Channel name mapping for reference (comments only):
# UC_x5XG1OV2P6uZZ5FSM9Ttw = Google Developers
# UCXuqSBlHAE6Xw-yeJA0Tunw = Linus Tech Tips
# UC4QZ_LsYcvcq7qOsOhpAX4A = CodeBullet
# UCld68syR8Wi-GY_n4CaoJGA = Brodie Robertson (Linux/Tech)
# UCVls1GmFKf6WlTraIb_IaJg = DistroTube
# UCsXVk37bltHxD1rDPwtNM8Q = Kurzgesagt ‚Äì In a Nutshell
# UC6nSFpj9HTCZ5t-N3Rm3-HA = Vsauce
# UCHnyfMqiRRG1u-2MsSQLbXA = Veritasium
# UC7_gcs09iThXybpVgjHZ_7g = PBS Space Time
# UCtYLUTtgS3k1Fg4y5tAhLbw = Statquest
# UCfM3zsQsOnfWNUppiycmBuw = Economics Explained
# UC0p5jTq6Xx_DosDFxVXnWaQ = Ben Felix
# UCZ4AMrDcNrfy3X6nsU8-rPg = Economics in Many Lessons
# UC-uWLJbmXH1KbCPHmtG-3MQ = Marginal Revolution University
# UCcunJy13-KFJNd_DkqkUeEg = CrashCourse Economics
```

### 6.2 Test Different Fields

```bash
# Test each field separately
python -c "
import asyncio
from utils.youtube_tool import YouTubeReporterTool, YouTubeToolParams
from core.config_service import ConfigService

async def test_field(field_name):
    tool = YouTubeReporterTool(ConfigService())
    params = YouTubeToolParams(field=field_name, days_back=7, max_videos_per_channel=1)
    result = await tool.execute(params)
    print(f'{field_name.upper()}: {result.videos_found} videos, {result.transcripts_obtained} transcripts')

async def main():
    for field in ['technology', 'science', 'economics']:
        await test_field(field)

asyncio.run(main())
"
```

## üì∞ Step 7: Integrate with News Generation

### 7.1 Test YouTube Tool in Reporter Agent

Create a test script to see how the YouTube tool works within the reporter agent workflow:

```python
# Create file: test_youtube_reporter.py
"""Test YouTube integration with reporter agent."""

import asyncio
from agents.agent_factory import AgentFactory
from agents.models.task_models import ReporterTask
from agents.task_execution_service import TaskExecutionService
from agents.types import ReporterField, TaskType
from core.config_service import ConfigService


async def test_youtube_in_reporter():
    """Test YouTube tool integration with reporter agent."""
    config_service = ConfigService()
    task_service = TaskExecutionService(config_service)
    factory = AgentFactory(config_service, task_service)

    # Create a technology reporter
    reporter = factory.create_reporter(ReporterField.TECHNOLOGY)

    # Create a task that should use YouTube data
    task = ReporterTask(
        name=TaskType.WRITE_STORY,
        field=ReporterField.TECHNOLOGY,
        description="Write a story about recent developments in AI and machine learning",
        guidelines="Use YouTube videos and transcripts to find the latest discussions and announcements"
    )

    print("ü§ñ Starting reporter task with YouTube integration...")
    result = await task_service.execute_reporter_task(task)

    if result.success:
        print("‚úÖ Task completed successfully!")
        print(f"üìÑ Story Title: {result.story.title}")
        print(f"üìù Story Length: {len(result.story.content)} characters")
        print(f"üîó Sources: {len(result.story.sources)} sources")

        # Check if YouTube was used
        youtube_sources = [s for s in result.story.sources if 'youtube.com' in s.url]
        if youtube_sources:
            print(f"üé• YouTube sources used: {len(youtube_sources)}")
            for source in youtube_sources[:2]:  # Show first 2
                print(f"   - {source.title}: {source.url}")
        else:
            print("‚ö†Ô∏è  No YouTube sources were used")
    else:
        print(f"‚ùå Task failed: {result.error}")


if __name__ == "__main__":
    asyncio.run(test_youtube_in_reporter())
```

### 7.2 Generate Full Newspaper with YouTube Integration

```bash
# Generate a newspaper that should now include YouTube sources
python generate_newpaper.py

# Check the output for YouTube sources
grep -r "youtube.com" data/newspapers/ || echo "No YouTube sources found"
```

## üîß Step 8: Advanced Configuration and Optimization

### 8.1 Add Caching Configuration

Update your environment configuration to include caching:

```bash
# In libs/.env.development
# YouTube caching settings (cache API responses to avoid rate limits)
YOUTUBE_CACHE_ENABLED=true
YOUTUBE_CACHE_DURATION_HOURS=6  # How long to cache video/channel data before refreshing
YOUTUBE_CACHE_MAX_VIDEOS=100    # Maximum number of videos to cache per field

# Performance settings
YOUTUBE_CONCURRENT_REQUESTS=3   # Max concurrent API requests
YOUTUBE_REQUEST_DELAY_MS=100    # Delay between requests to avoid rate limiting
```

### 8.2 Create Channel Management Script

Create a utility script to manage and validate YouTube channels:

```python
# Create file: manage_youtube_channels.py
"""Utility script for managing YouTube channels."""

import asyncio
from core.config_service import ConfigService
from utils.youtube_service import YouTubeService
from utils.youtube_models import YouTubeField


async def validate_channels():
    """Validate all configured YouTube channels."""
    config_service = ConfigService()
    youtube_service = YouTubeService(config_service)

    # Get all field configurations from environment
    fields = [YouTubeField.TECHNOLOGY, YouTubeField.SCIENCE, YouTubeField.ECONOMICS, YouTubeField.SPORTS]

    print("üîç Validating YouTube channels...")

    for field in fields:
        channel_ids = youtube_service.get_channels_for_field(field)

        if not channel_ids:
            print(f"\nüìÇ {field.upper()} Field: No channels configured")
            continue

        print(f"\nüìÇ {field.upper()} Field:")

        for channel_id in channel_ids:
            try:
                channel_info = await youtube_service._get_channel_info(channel_id)
                if channel_info:
                    print(f"  ‚úÖ {channel_info.title} ({channel_info.subscriber_count:,} subscribers)")
                else:
                    print(f"  ‚ùå Channel not found: {channel_id}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Error checking {channel_id}: {e}")


async def find_recent_videos():
    """Find recent videos from all configured channels."""
    config_service = ConfigService()
    youtube_service = YouTubeService(config_service)

    # Get all field configurations from environment
    fields = [YouTubeField.TECHNOLOGY, YouTubeField.SCIENCE, YouTubeField.ECONOMICS, YouTubeField.SPORTS]

    print("üì∫ Recent videos from all channels:")

    for field in fields:
        channel_ids = youtube_service.get_channels_for_field(field)

        if not channel_ids:
            print(f"\nüéØ {field.upper()} Field: No channels configured")
            continue

        print(f"\nüéØ {field.upper()} Field:")

        for channel_id in channel_ids:
            try:
                from datetime import datetime, timedelta
                since_date = datetime.now() - timedelta(days=3)

                videos = await youtube_service._get_channel_videos(channel_id, 2, since_date)

                if videos:
                    channel_name = videos[0].channel_title
                    print(f"  üì∫ {channel_name}:")
                    for video in videos:
                        print(f"    - {video.title} ({video.view_count:,} views)")
                else:
                    channel_info = await youtube_service._get_channel_info(channel_id)
                    channel_name = channel_info.title if channel_info else channel_id
                    print(f"  üì∫ {channel_name}: No recent videos")

            except Exception as e:
                print(f"  ‚ö†Ô∏è  Error: {e}")


async def main():
    """Main function to run channel management tasks."""
    print("üé¨ YouTube Channel Management")
    print("=" * 40)

    await validate_channels()
    print("\n" + "=" * 40)
    await find_recent_videos()


if __name__ == "__main__":
    asyncio.run(main())
```

### 8.3 Run Channel Management

```bash
# Validate and check all configured channels
python manage_youtube_channels.py
```

## üéØ Step 9: Verification and Testing

### 9.1 Complete Integration Test

Create a comprehensive test to verify everything works together:

```python
# Create file: test_complete_youtube_integration.py
"""Complete integration test for YouTube functionality."""

import asyncio
from datetime import datetime
from core.config_service import ConfigService
from utils.youtube_tool import YouTubeReporterTool, YouTubeToolParams
from utils.youtube_models import YouTubeField


async def comprehensive_test():
    """Run comprehensive YouTube integration tests."""
    print("üß™ YouTube Integration Test Suite")
    print("=" * 50)

    config_service = ConfigService()
    youtube_tool = YouTubeReporterTool(config_service)

    # Test 1: Basic functionality
    print("\n1Ô∏è‚É£ Testing basic YouTube search...")
    params = YouTubeToolParams(
        field=YouTubeField.TECHNOLOGY,
        operation="transcribe",
        days_back=7,
        max_videos_per_channel=2
    )

    result = await youtube_tool.execute(params)

    if result.success:
        print(f"   ‚úÖ Found {result.videos_found} videos")
        print(f"   ‚úÖ Got {result.transcripts_obtained} transcripts")
    else:
        print(f"   ‚ùå Failed: {result.error}")
        return

    # Test 2: Custom channels
    print("\n2Ô∏è‚É£ Testing custom channel search...")
    custom_params = YouTubeToolParams(
        channel_ids=["UC_x5XG1OV2P6uZZ5FSM9Ttw"],  # Google Developers
        operation="topics",
        days_back=14,
        max_videos_per_channel=1
    )

    custom_result = await youtube_tool.execute(custom_params)

    if custom_result.success:
        print(f"   ‚úÖ Custom search found {custom_result.videos_found} videos")
    else:
        print(f"   ‚ùå Custom search failed: {custom_result.error}")

    # Test 3: All fields
    print("\n3Ô∏è‚É£ Testing all fields...")
    for field in [YouTubeField.TECHNOLOGY, YouTubeField.SCIENCE, YouTubeField.ECONOMICS, YouTubeField.SPORTS]:
        field_params = YouTubeToolParams(
            field=field,
            operation="topics",
            days_back=5,
            max_videos_per_channel=1
        )

        field_result = await youtube_tool.execute(field_params)

        if field_result.success:
            print(f"   ‚úÖ {field}: {field_result.videos_found} videos")
        else:
            print(f"   ‚ùå {field}: {field_result.error}")

    # Test 4: Data quality check
    print("\n4Ô∏è‚É£ Testing data quality...")
    if result.detailed_results:
        first_video = result.detailed_results[0]
        required_fields = ["title", "channel", "url", "published"]

        missing_fields = [field for field in required_fields if field not in first_video]

        if not missing_fields:
            print("   ‚úÖ All required fields present")
        else:
            print(f"   ‚ö†Ô∏è  Missing fields: {missing_fields}")

        if "transcript" in first_video and len(first_video["transcript"]) > 100:
            print("   ‚úÖ Transcript quality good")
        else:
            print("   ‚ö†Ô∏è  Transcript missing or too short")

    print("\nüéâ Integration test completed!")
    print(f"üìä Total videos found across all tests: {result.videos_found + custom_result.videos_found}")


if __name__ == "__main__":
    asyncio.run(comprehensive_test())
```

### 9.2 Run Complete Test Suite

```bash
# Run the comprehensive test
python test_complete_youtube_integration.py
```

## üéâ Step 10: Lab Completion and Next Steps

### 10.1 Verify Lab Completion

Check that you have successfully:

```bash
# 1. Check YouTube tool is registered
python -c "
from agents.reporter_agent.reporter_tools import ReporterToolRegistry
from core.config_service import ConfigService
registry = ReporterToolRegistry(ConfigService())
tools = registry.get_tool_descriptions()
if 'youtube_search' in tools:
    print('‚úÖ YouTube tool registered successfully')
else:
    print('‚ùå YouTube tool not found in registry')
"

# 2. Check configuration
python -c "
from core.config_service import ConfigService
config = ConfigService()
api_key = config.get('youtube.api_key')
channels = config.get('youtube_channels', {})
print(f'‚úÖ API Key configured: {bool(api_key)}')
print(f'‚úÖ Channels configured: {len(channels)} fields')
"

# 3. Generate newspaper with YouTube integration
python generate_newpaper.py
```

### 10.2 Understanding What You Built

You have successfully:

1. **üîß Created a YouTube Data Source**
   - Integrated YouTube Data API v3
   - Implemented video search and metadata extraction
   - Added transcript fetching capabilities

2. **üõ†Ô∏è Built a Reporter Tool**
   - Created `YouTubeReporterTool` following the BaseTool pattern
   - Implemented structured parameters and results
   - Added field-specific channel configuration

3. **üîó Integrated with Agent Workflow**
   - Registered the tool with `ReporterToolRegistry`
   - Made it available to all reporter agents
   - Enabled automatic usage in story research

4. **‚öôÔ∏è Configured Multi-Field Support**
   - Set up technology, science, and economics channels
   - Implemented custom channel override capability
   - Added comprehensive error handling

### 10.3 Key Implementation Details

**üîß Architecture Decisions:**
- **Two-Mode Operation**: Full mode (with API) vs Transcript-only mode (no API)
- **Direct Channel IDs**: Using channel IDs instead of usernames to minimize API calls
- **Separate Libraries**: `youtube-transcript-api` for transcripts (quota-free) + `google-api-python-client` for metadata
- **Pydantic Models**: Full type safety with `YouTubeVideo`, `VideoTranscript`, `YouTubeToolParams`, etc.
- **Source Tracking**: Automatic `StorySource` creation for news attribution

**üéØ Key Learning Points:**
- **Tool Architecture**: How to extend the system with new data sources
- **API Integration**: Working with external APIs and handling authentication
- **Configuration Management**: Using the ConfigService for secrets and settings
- **Agent Integration**: How tools are discovered and used by agents
- **Error Handling**: Robust error handling for external service dependencies
- **Quota Management**: Strategies for working within API limitations
- **Type Safety**: Using Pydantic models throughout the integration

### 10.4 Next Steps and Extensions

**Potential Enhancements:**
1. **Enhanced Transcript Processing**: Add transcript cleaning and summarization
2. **Video Analysis**: Add video thumbnail analysis using vision models
3. **Trending Detection**: Implement trending topic detection across channels
4. **Content Filtering**: Add content quality and relevance filtering
5. **Caching Layer**: Implement Redis caching for better performance
6. **Real-time Updates**: Add webhook support for real-time video notifications
7. **Multi-language Support**: Handle transcripts in different languages

**Other Data Sources to Add:**
- Twitter/X API integration
- Reddit API for community discussions
- RSS feed aggregation
- News API integration
- Podcast transcript analysis

## üí° Transcript-Only Mode (No API Key Required)

Our implementation supports a **transcript-only mode** that works without any YouTube API key:

### ‚úÖ What Works Without API Key:
```python
# Test transcript-only functionality
from utils.youtube_tool import YouTubeReporterTool, YouTubeToolParams

# Extract transcripts from specific video IDs (completely free)
params = YouTubeToolParams(
    operation="transcribe",
    specific_video_ids=["dQw4w9WgXcQ", "jNQXAC9IVRw", "9bZkp7q19f0"]
)

result = await youtube_tool.execute(params)
# ‚úÖ Success: True, Transcripts: 3, Sources: 3
```

### üîß Test Transcript-Only Mode:
```bash
# Run the transcript-only test
python test_youtube_transcript_only.py

# Expected output:
# ‚úÖ YouTube Integration Lab 02 - COMPLETED SUCCESSFULLY!
# üéØ Key Features Implemented:
#    ‚Ä¢ YouTube transcript extraction (quota-free)
#    ‚Ä¢ Reporter tool integration
#    ‚Ä¢ Source tracking for news generation
```

### üéØ Alternative Video Discovery Methods:
1. **Manual Curation**: Provide specific video IDs from trending content
2. **RSS Feeds**: Use YouTube channel RSS feeds (no API key needed)
3. **Web Scraping**: Extract video IDs from channel pages
4. **Third-party APIs**: Use alternative video discovery services

**This approach is completely free and requires no API keys!**

## üß™ Testing Your Implementation

We've created several test files to validate your YouTube integration:

### 1. Comprehensive Integration Test
```bash
python test_youtube_integration.py
```
- Tests topic extraction from configured channels
- Tests transcript functionality
- Tests all field configurations (technology, science, economics, sports)
- **Note**: May hit API quota limits with username URLs

### 2. Transcript-Only Test (No API Quota)
```bash
python test_youtube_transcript_only.py
```
- Tests transcript extraction from specific video IDs
- Tests reporter tool integration
- Tests source creation and tracking
- **Guaranteed to work without API quota issues**

### 3. Simple Registry Test
```bash
python test_youtube_simple.py
```
- Tests YouTube tool registration in reporter registry
- Tests source conversion functionality
- Quick validation of core integration

### 4. Reporter Agent Integration Test
```bash
python test_youtube_reporter.py
```
- Tests full integration with reporter agents
- Tests story generation with YouTube sources
- **Note**: Requires working LLM configuration

## üèÜ Congratulations!

You have successfully completed Lab 2! You've learned how to:
- ‚úÖ Extend the BobTimes system with new data sources
- ‚úÖ Create custom tools following the established patterns
- ‚úÖ Integrate external APIs with proper error handling
- ‚úÖ Configure field-specific data source mappings
- ‚úÖ Test and validate your integration thoroughly
- ‚úÖ Handle API quota limitations gracefully
- ‚úÖ Implement transcript-only mode for quota-free operation

Your YouTube integration is now part of the news generation pipeline, providing rich video content and transcripts to enhance story research and writing.

## üìã Source Integration Verification

### How YouTube Sources Appear in Stories

When your YouTube tool is used by reporter agents, the sources will automatically be tracked and appear in the final stories:

```json
{
  "title": "Latest AI Developments in 2024",
  "content": "Based on recent discussions from leading tech channels...",
  "sources": [
    {
      "url": "https://www.youtube.com/watch?v=abc123",
      "title": "Google's New AI Breakthrough",
      "summary": "YouTube video from Google Developers: Discussion of the latest AI model improvements and their implications for developers...",
      "accessed_at": "2024-01-15T10:30:00Z"
    },
    {
      "url": "https://www.youtube.com/watch?v=def456",
      "title": "Tech Industry Analysis",
      "summary": "YouTube video from Linus Tech Tips: Analysis of recent tech trends and their impact on consumers...",
      "accessed_at": "2024-01-15T10:30:00Z"
    }
  ]
}
```

### Source Tracking Benefits

‚úÖ **Automatic Attribution**: YouTube videos are properly cited as sources
‚úÖ **Content Traceability**: Readers can verify information by checking original videos
‚úÖ **SEO Benefits**: Rich source metadata improves content discoverability
‚úÖ **Editorial Oversight**: Editors can review source quality and relevance
‚úÖ **Compliance**: Proper attribution meets journalistic standards

### Integration with Existing Workflow

The YouTube tool seamlessly integrates with the existing source tracking system:

1. **Search Tool** ‚Üí Web search results as sources
2. **Scraper Tool** ‚Üí Scraped web pages as sources
3. **YouTube Tool** ‚Üí Video content and transcripts as sources
4. **All Sources** ‚Üí Automatically tracked in story metadata

**üîÑ Ready for Lab 3?** The next lab will focus on advanced agent customization and workflow optimization!
